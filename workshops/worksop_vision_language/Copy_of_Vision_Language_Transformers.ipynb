{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-05-07T14:33:38.104232Z",
     "iopub.execute_input": "2023-05-07T14:33:38.104691Z",
     "iopub.status.idle": "2023-05-07T14:33:38.134970Z",
     "shell.execute_reply.started": "2023-05-07T14:33:38.104658Z",
     "shell.execute_reply": "2023-05-07T14:33:38.133818Z"
    },
    "trusted": true,
    "id": "28wWSKTGMVIg"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:15:09.930998Z",
     "iopub.execute_input": "2023-05-07T22:15:09.931368Z",
     "iopub.status.idle": "2023-05-07T22:15:09.942520Z",
     "shell.execute_reply.started": "2023-05-07T22:15:09.931335Z",
     "shell.execute_reply": "2023-05-07T22:15:09.941602Z"
    },
    "trusted": true,
    "id": "95ne0u9DMVIl"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertModel, BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "embedding_matrix = bert.embeddings.word_embeddings.weight"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:15:11.024571Z",
     "iopub.execute_input": "2023-05-07T22:15:11.024919Z",
     "iopub.status.idle": "2023-05-07T22:15:30.185768Z",
     "shell.execute_reply.started": "2023-05-07T22:15:11.024889Z",
     "shell.execute_reply": "2023-05-07T22:15:30.184758Z"
    },
    "trusted": true,
    "id": "XnZoRnEVMVIm",
    "outputId": "2f69f883-e31c-458f-fb36-d2deb7199853",
    "colab": {
     "referenced_widgets": [
      "88281a2468254dc1beb419c52d0c6ce6",
      "8311fa2a57c945d38ad7fe331a8f4a4d",
      "85cc373540c34372b1054a5e81812f3c",
      "1870366211b141aaa7c495e32a78195f",
      "5113f1e4f8bb4b3ebc992ff95a52a049"
     ]
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88281a2468254dc1beb419c52d0c6ce6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8311fa2a57c945d38ad7fe331a8f4a4d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85cc373540c34372b1054a5e81812f3c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1870366211b141aaa7c495e32a78195f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5113f1e4f8bb4b3ebc992ff95a52a049"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "PAD_ID = 0\n",
    "CLS_ID = 101\n",
    "device = \"cuda:0\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:21:47.822170Z",
     "iopub.execute_input": "2023-05-07T22:21:47.822574Z",
     "iopub.status.idle": "2023-05-07T22:21:47.828977Z",
     "shell.execute_reply.started": "2023-05-07T22:21:47.822543Z",
     "shell.execute_reply": "2023-05-07T22:21:47.827304Z"
    },
    "trusted": true,
    "id": "XJ2KA8AwMVIn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_text = \"Here is some text to encode\"\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "# you can get BERT embeddings like this:\n",
    "embedding_matrix[input_ids].shape, input_ids"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:21:48.895846Z",
     "iopub.execute_input": "2023-05-07T22:21:48.896200Z",
     "iopub.status.idle": "2023-05-07T22:21:48.904164Z",
     "shell.execute_reply.started": "2023-05-07T22:21:48.896172Z",
     "shell.execute_reply": "2023-05-07T22:21:48.903229Z"
    },
    "trusted": true,
    "id": "LqHgce_NMVIo",
    "outputId": "580c14d9-214b-4458-8a80-029fe7369f4d"
   },
   "execution_count": null,
   "outputs": [
    {
     "execution_count": 30,
     "output_type": "execute_result",
     "data": {
      "text/plain": "(torch.Size([9, 768]), [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Let's begin !\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import json\n",
    "import csv \n",
    "import torch\n",
    "\n",
    "class VQADataset(Dataset):\n",
    "\n",
    "    def __init__(self, split_path):\n",
    "        image_features_path = \"/kaggle/input/minivqaiust/image_features.pickle\"\n",
    "        answers_list_path = \"/kaggle/input/minivqaiust/answer_list.txt\"\n",
    "        image2questions_path = \"/kaggle/input/minivqaiust/image_question.json\"\n",
    "        \n",
    "        ## Read image features, use pickle!\n",
    "        with open(image_features_path, 'rb') as f:\n",
    "            ### YOUR CODE HERE\n",
    "            self.image_features = None\n",
    "            ### YOUR CODE HERE\n",
    "        \n",
    "        ##sample: self.question2img[q_id] = img_id\n",
    "        self.question2img = {}\n",
    "        \n",
    "        ##sample: self.questions[q_id] = {\"text\" : q_text, \"tokenized\" : tokenized_question}\n",
    "        ## tokenization: tokenizer.encode(sentence)\n",
    "        self.questions = {}\n",
    "        \n",
    "        with open(image2questions_path, 'r') as f:\n",
    "            ## YOUR CODE HERE\n",
    "            ## Load json file (image2questions)\n",
    "            data = None\n",
    "            \n",
    "            ## retrieve requested values \"self.question2img\", \"self.questions\" from givenn json\n",
    "            ## ~ 6 lines\n",
    "            None\n",
    "            ### YOUR CODE HERE\n",
    "        \n",
    "        self.possible_answers = []\n",
    "        with open(answers_list_path, 'r') as f:\n",
    "            ## read answers list from text file, save them in an array\n",
    "            self.possible_answers = f.read().split()\n",
    "        \n",
    "        ## sample: self.data[idx] = q_id\n",
    "        self.data = []\n",
    "        ## sample: self.labels[idx] = 4\n",
    "        self.labels = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## load data from \"split_path\", fill self.data and self.labels as requested! take a look at train.csv\n",
    "        # https://docs.python.org/3/library/csv.html#csv.DictReader\n",
    "        with open(split_path, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                self.data.append(int(row['question_id']))\n",
    "                if (row['label'] is not None):\n",
    "                    self.labels.append(int(row['label']))\n",
    "                else:\n",
    "                    self.labels = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This method returns tuple of (question_id, image_features (Tensor), tokenized_question (Tensor), label\n",
    "        \n",
    "        Note: label can be None!\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        q_id = self.data[idx]\n",
    "        ### YOUR CODE HERE\n",
    "        ## WARNING: while making tensors, DO NOT FORGET TO USE .to(device) at the end!\n",
    "        \n",
    "        q_tokenized = None\n",
    "        img_id = None\n",
    "        label = None\n",
    "        if(None):\n",
    "            label = None\n",
    "        return None\n",
    "        ### YOUR CODE HERE\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:21:49.418928Z",
     "iopub.execute_input": "2023-05-07T22:21:49.419318Z",
     "iopub.status.idle": "2023-05-07T22:21:49.430649Z",
     "shell.execute_reply.started": "2023-05-07T22:21:49.419282Z",
     "shell.execute_reply": "2023-05-07T22:21:49.429317Z"
    },
    "trusted": true,
    "id": "xuJTlmLAMVIo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "        Batch post processing, we can pad questions! \n",
    "        returns q_ids, images (Tensor), questions(Tensor), labels (Tensor)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    questions = []\n",
    "    labels = []\n",
    "    q_ids = []\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    ## WARNING: while making tensors, DO NOT FORGET TO USE .to(device) at the end!\n",
    "    for q_id, img, q_tokens, label in batch:\n",
    "        None\n",
    "    \n",
    "    ### Stack images into one tensor\n",
    "    ## torch.stack, shape must be (batch_size, img_features)\n",
    "    images = None\n",
    "    \n",
    "    ## stack labels if they're not None, else make labels None!\n",
    "    labels = None\n",
    "    \n",
    "    ## pad questions, shape must be (batch_size, longest_sentence)\n",
    "    ## https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n",
    "    questions = None\n",
    "    \n",
    "    \n",
    "    return None"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:21:51.283003Z",
     "iopub.execute_input": "2023-05-07T22:21:51.283401Z",
     "iopub.status.idle": "2023-05-07T22:21:51.289776Z",
     "shell.execute_reply.started": "2023-05-07T22:21:51.283368Z",
     "shell.execute_reply": "2023-05-07T22:21:51.288884Z"
    },
    "trusted": true,
    "id": "qUusYyS7MVIq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dset = VQADataset(\"/kaggle/input/minivqaiust/train.csv\")\n",
    "data_loader_train = DataLoader(dset, collate_fn=collate_batch, batch_size=32)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:21:51.643738Z",
     "iopub.execute_input": "2023-05-07T22:21:51.644119Z",
     "iopub.status.idle": "2023-05-07T22:21:51.723301Z",
     "shell.execute_reply.started": "2023-05-07T22:21:51.644088Z",
     "shell.execute_reply": "2023-05-07T22:21:51.722309Z"
    },
    "trusted": true,
    "id": "fWtX_zTFMVIq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "## Nothing, just look =)))\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"Positional encoding class pulled from the PyTorch documentation tutorial\n",
    "    on Transformers for seq2seq models:\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\\\n",
    "                             * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:21:52.626673Z",
     "iopub.execute_input": "2023-05-07T22:21:52.627011Z",
     "iopub.status.idle": "2023-05-07T22:21:52.635181Z",
     "shell.execute_reply.started": "2023-05-07T22:21:52.626982Z",
     "shell.execute_reply": "2023-05-07T22:21:52.634325Z"
    },
    "trusted": true,
    "id": "JDCXP41CMVIr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "#The most interesting part!\n",
    "\n",
    "class VQA_Simple(nn.Module):\n",
    "    def __init__(self, dropout, text_hidden_size, n_layers, n_heads, image_hidden_size, n_outputs):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.d_model = text_hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.image_hidden_size = image_hidden_size\n",
    "        self.PAD = PAD_ID\n",
    "        \n",
    "        self.embedding_matrix = bert.embeddings.word_embeddings.weight\n",
    "        \n",
    "        \n",
    "        ##initilize TransformerEncoderLayer\n",
    "        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
    "        encoder_layer = None\n",
    "        \n",
    "        ##initilize TransformerEncoder\n",
    "        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html\n",
    "        self.t_encoder = None\n",
    "        \n",
    "        ##if you looke enough, you can initilize positional encoder!!\n",
    "        self.pe = None\n",
    "        \n",
    "        ##initilize TransformerDecoderLayer\n",
    "        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html\n",
    "        decoder_layer = None\n",
    "        \n",
    "        ##initilize TransformerDecoder\n",
    "        ##https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html\n",
    "        self.t_decoder = None\n",
    "        \n",
    "        ##Linear output, recieves concatenation of text and image features, outputs final answer!\n",
    "        self.linear = None\n",
    "        \n",
    "    def forward(self, images, input_ids):\n",
    "        ##images shape: (batch_size, img_features)\n",
    "        ##input_ids shape: (batch_size, sequence_len)\n",
    "        b_size = images.shape[0]\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        ## Calculate masks, shape: (batch_size, sequence_len)\n",
    "        src_key_mask = None\n",
    "        ##embeddings of the given input_ids, extracted from self.embedding_matrix\n",
    "        ##shape should be (sequence_len, batch_size, text_embedding_features)\n",
    "        embeddings = None\n",
    "        \n",
    "        ##Positional embeddings\n",
    "        ##shape should be (sequence_len, batch_size, text_embedding_features)\n",
    "        positional_embeddings = None\n",
    "        \n",
    "        ## feed positinal_embeddings to the encoder!\n",
    "        ## output shape should be (sequence_len, batch_size, d_model)\n",
    "        ## additional args:  src_key_padding_mask\n",
    "        encoder_output = None\n",
    "        \n",
    "        ##(batch_size, 1)\n",
    "        tgt = torch.tensor([CLS_ID] * b_size).unsqueeze(1).to(device)\n",
    "        ##(batch_size, 1)\n",
    "        tgt_key_padding_mask = (tgt == self.PAD)\n",
    "        \n",
    "        ##embeddings of the given input_ids, extracted from self.embedding_matrix\n",
    "        ##shape should be (1, batch_size, text_embedding_features)\n",
    "        tgt_embeddings = None\n",
    "        \n",
    "\n",
    "        # target attention masks to avoid future tokens in our predictions\n",
    "        # Adapted from PyTorch source code:\n",
    "        # https://github.com/pytorch/pytorch/blob/176174a68ba2d36b9a5aaef0943421682ecc66d4/torch/nn/modules/transformer.py#L130\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(1).to(device)\n",
    "        \n",
    "        ## Positional embedding \n",
    "        tgt_positions = None\n",
    "        \n",
    "        output = self.t_decoder(tgt=None, \n",
    "                                memory=None,\n",
    "                                tgt_mask=None,\n",
    "                                tgt_key_padding_mask = None, \n",
    "                                memory_key_padding_mask = None) ##(1, batch_size, text_embedding_features)\n",
    "        \n",
    "        \n",
    "        \n",
    "        output_text = output.permute(1, 0, 2).squeeze(1) ## (batch_size, text_embedding_features)\n",
    "        \n",
    "        ##https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "        #concatenate text output and image features\n",
    "        concatenated = None\n",
    "        \n",
    "        \n",
    "        y = self.linear(concatenated)\n",
    "        \n",
    "        return y\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:21:53.375964Z",
     "iopub.execute_input": "2023-05-07T22:21:53.376841Z",
     "iopub.status.idle": "2023-05-07T22:21:53.388269Z",
     "shell.execute_reply.started": "2023-05-07T22:21:53.376798Z",
     "shell.execute_reply": "2023-05-07T22:21:53.387301Z"
    },
    "trusted": true,
    "id": "k6oCv94cMVIs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "model = VQA_Simple(dropout=0.1, \n",
    "                   text_hidden_size=768, \n",
    "                   n_layers=2, \n",
    "                   n_heads=6, \n",
    "                   image_hidden_size=512, \n",
    "                   n_outputs=10).cuda()\n",
    "lr=1e-4\n",
    "epochs = 25\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (q_ids, images, questions, labels) in enumerate(pbar := tqdm(data_loader_train, total=len(data_loader_train))):\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, questions)\n",
    "        \n",
    "        loss = criterion(output.cpu(), labels)\n",
    "        running_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        log_interval = 5\n",
    "        pbar.set_postfix(loss=running_loss/(i+1))\n",
    "        "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:21:54.174941Z",
     "iopub.execute_input": "2023-05-07T22:21:54.175305Z",
     "iopub.status.idle": "2023-05-07T22:22:19.930473Z",
     "shell.execute_reply.started": "2023-05-07T22:21:54.175273Z",
     "shell.execute_reply": "2023-05-07T22:22:19.929482Z"
    },
    "trusted": true,
    "id": "lOc5yHHbMVIt",
    "outputId": "be44a560-3ba7-45f0-dbae-ac767237e84d"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "Epoch 0: 100%|██████████| 25/25 [00:01<00:00, 24.01it/s, loss=tensor(2.4785, grad_fn=<DivBackward0>)]\nEpoch 1: 100%|██████████| 25/25 [00:00<00:00, 25.17it/s, loss=tensor(2.3109, grad_fn=<DivBackward0>)]\nEpoch 2: 100%|██████████| 25/25 [00:01<00:00, 25.00it/s, loss=tensor(2.2884, grad_fn=<DivBackward0>)]\nEpoch 3: 100%|██████████| 25/25 [00:01<00:00, 24.97it/s, loss=tensor(2.2491, grad_fn=<DivBackward0>)]\nEpoch 4: 100%|██████████| 25/25 [00:01<00:00, 24.97it/s, loss=tensor(1.9947, grad_fn=<DivBackward0>)]\nEpoch 5: 100%|██████████| 25/25 [00:01<00:00, 24.81it/s, loss=tensor(1.5369, grad_fn=<DivBackward0>)]\nEpoch 6: 100%|██████████| 25/25 [00:01<00:00, 24.77it/s, loss=tensor(1.2724, grad_fn=<DivBackward0>)]\nEpoch 7: 100%|██████████| 25/25 [00:01<00:00, 23.21it/s, loss=tensor(1.0289, grad_fn=<DivBackward0>)]\nEpoch 8: 100%|██████████| 25/25 [00:00<00:00, 25.09it/s, loss=tensor(1.0160, grad_fn=<DivBackward0>)]\nEpoch 9: 100%|██████████| 25/25 [00:00<00:00, 25.24it/s, loss=tensor(0.9249, grad_fn=<DivBackward0>)]\nEpoch 10: 100%|██████████| 25/25 [00:01<00:00, 24.98it/s, loss=tensor(0.7475, grad_fn=<DivBackward0>)]\nEpoch 11: 100%|██████████| 25/25 [00:00<00:00, 25.00it/s, loss=tensor(0.8257, grad_fn=<DivBackward0>)]\nEpoch 12: 100%|██████████| 25/25 [00:01<00:00, 23.04it/s, loss=tensor(0.7327, grad_fn=<DivBackward0>)]\nEpoch 13: 100%|██████████| 25/25 [00:01<00:00, 21.98it/s, loss=tensor(0.6124, grad_fn=<DivBackward0>)]\nEpoch 14: 100%|██████████| 25/25 [00:00<00:00, 25.24it/s, loss=tensor(0.5824, grad_fn=<DivBackward0>)]\nEpoch 15: 100%|██████████| 25/25 [00:00<00:00, 25.05it/s, loss=tensor(0.5477, grad_fn=<DivBackward0>)]\nEpoch 16: 100%|██████████| 25/25 [00:00<00:00, 25.03it/s, loss=tensor(0.4384, grad_fn=<DivBackward0>)]\nEpoch 17: 100%|██████████| 25/25 [00:01<00:00, 23.69it/s, loss=tensor(0.4402, grad_fn=<DivBackward0>)]\nEpoch 18: 100%|██████████| 25/25 [00:00<00:00, 25.07it/s, loss=tensor(0.3990, grad_fn=<DivBackward0>)]\nEpoch 19: 100%|██████████| 25/25 [00:00<00:00, 25.10it/s, loss=tensor(0.3795, grad_fn=<DivBackward0>)]\nEpoch 20: 100%|██████████| 25/25 [00:00<00:00, 25.20it/s, loss=tensor(0.4328, grad_fn=<DivBackward0>)]\nEpoch 21: 100%|██████████| 25/25 [00:00<00:00, 25.15it/s, loss=tensor(0.4553, grad_fn=<DivBackward0>)]\nEpoch 22: 100%|██████████| 25/25 [00:00<00:00, 25.20it/s, loss=tensor(0.4115, grad_fn=<DivBackward0>)]\nEpoch 23: 100%|██████████| 25/25 [00:01<00:00, 24.98it/s, loss=tensor(0.4372, grad_fn=<DivBackward0>)]\nEpoch 24: 100%|██████████| 25/25 [00:00<00:00, 25.22it/s, loss=tensor(0.4180, grad_fn=<DivBackward0>)]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(data_loader, net):\n",
    "    predicts = []\n",
    "    ids = []\n",
    "    net.eval()\n",
    "    for i, (q_ids, images, questions, _) in enumerate(pbar := tqdm(data_loader, total=len(data_loader))):\n",
    "        outputs = net(images, questions)\n",
    "        outputs = torch.argmax(outputs, dim=1)\n",
    "        predicts.extend(outputs.cpu().tolist())\n",
    "        ids.extend(q_ids)\n",
    "    return predicts, ids\n",
    "\n",
    "\n",
    "test_dset = VQADataset(\"/kaggle/input/minivqaiust/test.csv\")\n",
    "data_loader_test = DataLoader(test_dset, collate_fn=collate_batch, batch_size=8)\n",
    "preds, ids = predict(data_loader_test, model)\n",
    "\n",
    "# with open(\"output.txt\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:22:28.304101Z",
     "iopub.execute_input": "2023-05-07T22:22:28.305092Z",
     "iopub.status.idle": "2023-05-07T22:22:28.503450Z",
     "shell.execute_reply.started": "2023-05-07T22:22:28.305056Z",
     "shell.execute_reply": "2023-05-07T22:22:28.502462Z"
    },
    "trusted": true,
    "id": "Nup5MFDNMVIu",
    "outputId": "03489d3a-f1eb-411d-9f87-898508408ca4"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 14/14 [00:00<00:00, 126.20it/s]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_data = {\"question_id\": [str(id) for id in ids], \"label\": preds}\n",
    "df = pd.DataFrame(output_data)\n",
    "df.to_csv(\"/kaggle/working/output.csv\", index=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-07T22:23:54.752224Z",
     "iopub.execute_input": "2023-05-07T22:23:54.752594Z",
     "iopub.status.idle": "2023-05-07T22:23:54.759619Z",
     "shell.execute_reply.started": "2023-05-07T22:23:54.752566Z",
     "shell.execute_reply": "2023-05-07T22:23:54.758498Z"
    },
    "trusted": true,
    "id": "yz4RZHhTMVIu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8uxPg67aMVIv"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
